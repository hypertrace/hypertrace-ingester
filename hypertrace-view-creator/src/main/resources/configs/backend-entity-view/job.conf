mainClass = org.hypertrace.core.viewcreator.pinot.PinotTableCreationTool

view.name = backendEntityView
view.output.schema.class = org.hypertrace.viewgenerator.api.BackendEntityView
view.output.schema.url = "host:port/myView"

kafka.brokerAddress = "bootstrap:9092"
kafka.topicName = backend-entity-view-events
kafka.partitions = 1
kafka.replicationFactor = 1

pinot.controllerHost = pinot-controller
pinot.controllerPort = 9000
pinot.timeColumn = start_time_millis
pinot.timeUnit = MILLISECONDS
pinot.dimensionColumns = [tenant_id, trace_id, span_id, backend_id, backend_host, backend_port, backend_protocol, backend_path, span_kind, exception_count, error_count, duration_millis, end_time_millis, num_calls, backend_name, backend_trace_id, display_name, tags__KEYS, tags__VALUES, status_code, status_message, status, caller_service_id, caller_api_id]
pinot.columnsMaxLength={}
pinot.metricColumns = []
pinot.invertedIndexColumns= []
pinot.tableName = backendEntityView
pinot.loadMode = MMAP
pinot.numReplicas = 1
pinot.retentionTimeValue = 5
pinot.retentionTimeUnit = DAYS
pinot.brokerTenant = defaultBroker
pinot.serverTenant = defaultServer
pinot.segmentAssignmentStrategy = BalanceNumSegmentAssignmentStrategy

pinot.streamConfigs =
{
    streamType: kafka,
    stream.kafka.consumer.type: LowLevel,
    stream.kafka.topic.name: backend-entity-view-events,
    stream.kafka.consumer.factory.class.name: "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
    stream.kafka.decoder.class.name: "org.apache.pinot.plugin.inputformat.avro.confluent.KafkaConfluentSchemaRegistryAvroMessageDecoder",
    stream.kafka.decoder.prop.schema.registry.rest.url: "http://schema-registry-service:8081",
    stream.kafka.hlc.zk.connect.string: "zookeeper:2181",
    stream.kafka.zk.broker.url: "zookeeper:2181",
    stream.kafka.broker.list: "bootstrap:9092",
    realtime.segment.flush.threshold.time: 3600000,
    realtime.segment.flush.threshold.size: 500000,
    stream.kafka.consumer.prop.auto.offset.reset: largest
}