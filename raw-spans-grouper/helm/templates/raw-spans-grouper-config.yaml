apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.rawSpansGrouperConfig.name }}
  labels:
    release: {{ .Release.Name }}
data:
  application.conf: |-
    kafka.streams.config = {
      # Core configs
      application.id = raw-spans-to-structured-traces-grouping-job
      bootstrap.servers = "{{ .Values.spanNormalizerConfig.kafkaStreamsConfig.bootstrapServers }}"
      schema.registry.url = "{{ .Values.spanNormalizerConfig.kafkaStreamsConfig.schemaRegistryUrl }}"
      value.subject.name.strategy = "io.confluent.kafka.serializers.subject.TopicRecordNameStrategy"
      # Core configs - For applications with state
      num.stream.threads = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.numStreamThreads }}"
      commit.interval.ms = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.commitIntervalMs }}"
      group.instance.id = ${?POD_NAME}
      cache.max.bytes.buffering = "{{ int .Values.rawSpansGrouperConfig.kafkaStreamsConfig.cacheMaxBytesBuffering }}"
      # Common client (prodcuer, consumer, admin) configs
      receive.buffer.bytes = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.receiveBufferBytes }}"
      send.buffer.bytes = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.sendBufferBytes }}"
      # Producer configs
      producer.acks = "{{ .Values.spanNormalizerConfig.kafkaStreamsConfig.producerAcks }}"
      producer.batch.size = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.producerBatchSize }}"
      producer.linger.ms = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.producerLingerMs }}"
      producer.compression.type = "{{ .Values.spanNormalizerConfig.kafkaStreamsConfig.producerCompressionType }}"
      producer.max.request.size = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.producerMaxRequestSize }}"
      producer.buffer.memory = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.producerBufferMemory }}"
      # Consumer configs
      consumer.max.partition.fetch.bytes = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.consumerMaxPartitionFetchBytes }}"
      consumer.max.poll.records = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.consumerMaxPollRecords }}"
      consumer.session.timeout.ms = "{{ int .Values.spanNormalizerConfig.kafkaStreamsConfig.consumerSessionTimeoutMs }}"
      # Changelog topic configs
      replication.factor = "{{ int .Values.rawSpansGrouperConfig.kafkaStreamsConfig.replicationFactor }}"
      topic.cleanup.policy = "delete,compact"
      # RocksDB state store configs
      state.dir = "/var/data/"
      rocksdb.block.cache.size = {{ int .Values.rawSpansGrouperConfig.kafkaStreamsConfig.rocksdbBlockCacheSize }}
      rocksdb.write.buffer.size = {{ int .Values.rawSpansGrouperConfig.kafkaStreamsConfig.rocksdbWriteBufferSize }}
      rocksdb.max.write.buffers = {{ int .Values.rawSpansGrouperConfig.kafkaStreamsConfig.rocksdbMaxWriteBuffers }}
      rocksdb.cache.index.and.filter.blocks = {{ .Values.rawSpansGrouperConfig.kafkaStreamsConfig.rocksdbCacheIndexAndFilterBlocks }}
      # Exception handler configs
      default.production.exception.handler = {{ .Values.rawSpansGrouperConfig.kafkaStreamsConfig.defaultProductionExceptionHandler }}
      ignore.production.exception.classes = {{ .Values.rawSpansGrouperConfig.kafkaStreamsConfig.ignoreProductionExceptionClasses }}
      # Others
      metrics.recording.level = "{{ .Values.spanNormalizerConfig.kafkaStreamsConfig.metricsRecordingLevel }}"
      {{- if .Values.spanNormalizerConfig.extraKafkaStreamsConfig }}
      {{- range $key,$value := .Values.spanNormalizerConfig.extraKafkaStreamsConfig }}
      {{ $key }} = {{ $value }}
      {{- end }}
      {{- end }}
    }

    span.groupby.session.window.interval = {{ .Values.rawSpansGrouperConfig.span.groupby.internal }}

    {{- if hasKey .Values.rawSpansGrouperConfig "metrics" }}
    metrics {
      reporter {
        names = {{- toJson .Values.rawSpansGrouperConfig.metrics.reporter.names | trim | nindent 12 }}
      }
    }
    {{- end }}
