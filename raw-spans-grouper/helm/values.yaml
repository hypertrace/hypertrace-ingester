#
# Note about Namespace
# --------------------
# It is deliberately left out here and using the helm -n or --namespace flag you can deploy your resources to the same
# namespace as the release. If you leave it out, your resources will be deployed to the default namespace.
# Also, not that the namespace you are deploying to should already exist otherwise the helm command will fail.
# You can always specify a different namespace for a resource by setting it directly in it's yaml file or
# making it configurable by defining it in this file.

###########
# Deployment
###########
name: raw-spans-grouper
replicaCount: 2
maxUnavailable: 0

image:
  repository: hypertrace/raw-spans-grouper
  pullPolicy: IfNotPresent

imagePullSecrets: []

nodeLabels: {}

tolerations: []

securityContext: {}

# This is defined in resources/configs/common/application.conf as service.admin.port
containerAdminPort: 8099

javaOpts: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"

livenessProbe:
  initialDelaySeconds: 10
  periodSeconds: 5

readinessProbe:
  initialDelaySeconds: 2
  periodSeconds: 5

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  requests:
    cpu: 0.2
    memory: 1536Mi
  limits:
    cpu: 1.0
    memory: 1536Mi

deploymentLabels:
  app: raw-spans-grouper

podLabels:
  app: raw-spans-grouper

podAnnotations: {}

# The Deployment Selector match labels are different from the pod labels. Note that they should be a subset of the pod
# labels. You append new labels to them but cannot remove labels. If you remove or modify the labels you will need to
# delete the existing deployment bearing the same name and then redeploy. This is the reason why they are separated from
# the pod labels. You can add and remove pod labels without having an effect on the deployment.
# Also, please use "apiVersion: apps/v1" instead of the deprecated "apiVersion: extensions/v1beta1" for the deployment
# apiVersion in the yaml file.
deploymentSelectorMatchLabels:
  app: raw-spans-grouper

statefulSetSelectorMatchLabels:
  app: raw-spans-grouper

serviceSelectorMatchLabels:
  app: raw-spans-grouper

volumeClaimTemplates:
  name: raw-spans-grouper-rocksdb
  storageClassName: kafka-streams-rocksdb
  storageRequestSize: 50Gi

jmx:
  enabled: true
  port: 7022
  opts: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port=7022 -Dcom.sun.management.jmxremote.rmi.port=7022 -Djava.rmi.server.hostname=127.0.0.1"

prometheus:
  jmx:
    enabled: true
    port: 7071
    image:
      repository: solsson/kafka-prometheus-jmx-exporter@sha256
      tag: 6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: "0.25"
        memory: "256Mi"



###########
# Config Maps
###########
rawSpansGrouperConfig:
  name: raw-spans-grouper-config
  kafka:
    streams:
      config:
        metricsRecordingLevel: INFO
        numStreamThreads: 4
        bootstrapServers: "bootstrap:9092"
        schemaRegistryUrl: "http://schema-registry-service:8081"
        cacheMaxBytesBuffering: 16777216 # 16MB
        commitIntervalMs: 5000 #default
        producerBatchSize: 16384 #default
        producerLingerMs: 0 #default
        rocksdbBlockCacheSize: 33554432
        rocksdbWriteBufferSize: 8388608
        rocksdbMaxWriteBuffers: 2
        rocksdbCacheIndexAndFilterBlocks: true
        sessionTimeoutMs: 300000
        producerMaxRequestSize: 10485760
        defaultProductionExceptionHandler: "org.hypertrace.core.kafkastreams.framework.exceptionhandlers.IgnoreProductionExceptionHandler"
        ignoreProductionExceptionClasses: "org.apache.kafka.common.errors.RecordTooLargeException"
        numStandbyReplicas: 0 # Standby replicas are costly and needed only for applications which are high latency sensitive.
                              # They will help incase task failovers. Required only on need basis.
        replicationFactor: 3
  span:
    groupby:
      internal: 30

logConfig:
  name: raw-spans-grouper-log-config
  monitorInterval: 30
  rootLogger:
    level: INFO
  appender:
    rolling:
      enabled: false

kafka-topic-creator:
  enabled: true
  jobName: structured-traces-from-raw-spans-kafka-topic-creator
  helmHook: pre-install,pre-upgrade
  kafka:
    topics:
      - name: structured-traces-from-raw-spans
        replicationFactor: 3
        partitions: 8
        configs:
          - retention.bytes=4294967296
          - retention.ms=259200000
  zookeeper:
    address: zookeeper:2181
  imagePullSecrets: []
  podAnnotations: {}
