service.name = raw-spans-grouper
service.admin.port = 8099

main.class = org.hypertrace.core.rawspansgrouper.RawSpansGrouper

span.type = rawSpan
input.topic = "raw-spans-from-jaeger-spans"
output.topic = "structured-traces-from-raw-spans"
precreate.topics = false
precreate.topics = ${?PRE_CREATE_TOPICS}

kafka.streams.config = {
  application.id = raw-spans-to-structured-traces-grouping-job
  state.dir = "/var/data/"
  num.stream.threads = 4
  num.stream.threads = ${?NUM_STREAM_THREADS}
  num.standby.replicas = 1
  replication.factor = 3
  replication.factor = ${?REPLICATION_FACTOR}
  topic.cleanup.policy = "delete,compact"

  producer.max.request.size = 10485760
  default.production.exception.handler = org.hypertrace.core.kafkastreams.framework.exceptionhandlers.IgnoreProductionExceptionHandler
  ignore.production.exception.classes = org.apache.kafka.common.errors.RecordTooLargeException

  bootstrap.servers = "localhost:9092"
  bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}

  schema.registry.url = "http://localhost:8081"
  schema.registry.url = ${?SCHEMA_REGISTRY_URL}

  rocksdb.config.setter = org.hypertrace.core.kafkastreams.framework.rocksdb.RocksDBStateStoreConfigSetter
  rocksdb.block.cache.size = 33554432
  rocksdb.write.buffer.size = 8388608
  rocksdb.max.write.buffers = 2
  rocksdb.cache.index.and.filter.blocks = true

  value.subject.name.strategy = "io.confluent.kafka.serializers.subject.TopicRecordNameStrategy"
}

span.groupby.session.window.interval = 30
span.groupby.session.window.interval = ${?SPAN_GROUPBY_SESSION_WINDOW_INTERVAL}

logger {
  names = ["file"]
  file {
    dir = "/var/logs/raw-spans-grouper"
  }
}

metrics.reporter {
  prefix = org.hypertrace.core.rawspansgrouper.RawSpansGrouper
  names = ["prometheus"]
  console.reportInterval = 30
}

dataflow.metriccollection.sampling.percent = 10.0
