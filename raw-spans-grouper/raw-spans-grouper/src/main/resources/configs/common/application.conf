service.name = raw-spans-grouper
service.admin.port = 8051

main.class = org.hypertrace.core.rawspansgrouper.RawSpansGrouper

span.type = rawSpan
input.topic = "jaeger-spans"
output.topic = "structured-traces-from-raw-spans"
bypass.output.topic = "structured-traces-from-raw-spans"
precreate.topics = false
precreate.topics = ${?PRE_CREATE_TOPICS}

kafka.streams.config = {
  application.id = raw-spans-to-structured-traces-grouping-job
  state.dir = "/var/data/"
  num.stream.threads = 4
  num.stream.threads = ${?NUM_STREAM_THREADS}
  num.standby.replicas = 0
  replication.factor = 3
  replication.factor = ${?REPLICATION_FACTOR}
  topic.cleanup.policy = "delete,compact"

  producer.max.request.size = 10485760
  default.production.exception.handler = org.hypertrace.core.kafkastreams.framework.exceptionhandlers.IgnoreProductionExceptionHandler
  ignore.production.exception.classes = org.apache.kafka.common.errors.RecordTooLargeException

  bootstrap.servers = "localhost:9092"
  bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}

  schema.registry.url = "http://localhost:8081"
  schema.registry.url = ${?SCHEMA_REGISTRY_URL}

  rocksdb.config.setter = org.hypertrace.core.kafkastreams.framework.rocksdb.RocksDBStateStoreConfigSetter
  rocksdb.block.cache.size = 33554432
  rocksdb.write.buffer.size = 8388608
  rocksdb.max.write.buffers = 2
  rocksdb.cache.index.and.filter.blocks = true

  value.subject.name.strategy = "io.confluent.kafka.serializers.subject.TopicRecordNameStrategy"
}

group.partitioner = {
  enabled = false
  service.host = localhost
  service.port = 50104
}

span.groupby.session.window.interval = 30
span.groupby.session.window.interval = ${?SPAN_GROUPBY_SESSION_WINDOW_INTERVAL}

logger {
  names = ["file"]
  file {
    dir = "/var/logs/raw-spans-grouper"
  }
}

metrics.reporter {
  prefix = org.hypertrace.core.rawspansgrouper.RawSpansGrouper
  names = ["prometheus"]
  console.reportInterval = 30
}

dataflow.metriccollection.sampling.percent = 10.0


clients = {
  config.service.config = {
    host = localhost
    port = 50101
  }
}

processor {
  defaultTenantId = ${?DEFAULT_TENANT_ID}
  late.arrival.threshold.duration = 365d

  # Configuration for dropping certain attributes that are captured by agent, but doesn't require in
  # the processing pipeline.
  #
  # allowed.attributes.prefixes : the list of prefixes that should match for which allowed keys
  # prefixed.matched.allowed.attributes : allowed keys from the subset of keys where prefix matched
  #
  # If either of config is empty allowed.attributes.prefixes or prefixed.matched.allowed.attributes,
  # it will not drop any attributes.
  # The above configuration doesn't impact if the key doesn't start with prefix.
  allowed.attributes.prefixes = []
  prefixed.matched.allowed.attributes = []
}

span.rules.exclude {
  cache = {
    refreshAfterWriteDuration = 3m
    expireAfterWriteDuration = 5m
  }
}

rate.limit.config = []
